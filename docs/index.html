<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shariar Vaez-Ghaemi">
<meta name="dcterms.date" content="2025-06-25">

<title>Finding features that persist across layers and models – …?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-45a984b55c978f8c4117b5b9aec951d9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">…?</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./post.html" aria-current="page"> 
<span class="menu-text">Mechninterp/Explainability</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#a-first-pass-approach-correlated-features" id="toc-a-first-pass-approach-correlated-features" class="nav-link" data-scroll-target="#a-first-pass-approach-correlated-features">A first-pass approach: correlated features</a>
  <ul class="collapse">
  <li><a href="#examples-of-correlated-features" id="toc-examples-of-correlated-features" class="nav-link" data-scroll-target="#examples-of-correlated-features">Examples of correlated features</a></li>
  </ul></li>
  <li><a href="#similarity-in-decoder-weights" id="toc-similarity-in-decoder-weights" class="nav-link" data-scroll-target="#similarity-in-decoder-weights">Similarity in decoder weights</a></li>
  <li><a href="#visualizing-representation-comparison" id="toc-visualizing-representation-comparison" class="nav-link" data-scroll-target="#visualizing-representation-comparison">Visualizing Representation Comparison</a></li>
  <li><a href="#model-diffing-with-decoder-weights" id="toc-model-diffing-with-decoder-weights" class="nav-link" data-scroll-target="#model-diffing-with-decoder-weights">Model diffing with decoder weights</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next steps</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Finding features that persist across layers and models</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Shariar Vaez-Ghaemi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 25, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In statistical prediction settings, it’s known that the best models are the ones with the most important features, rather than the most complicated architectures. Choosing good features for your prediction model can be easy, thanks to regularization techniques and the expressiveness of deep architectures.</p>
<p>But while feature engineering has become an abandoned art, feature awareness is still crucial. In the race to understand the computational paths taken by a transformer in computing a next-token prediction, mechanistic interpretability researchers have put effort into extracting concept subspaces within the intermediate representations of these transformer architectures. In particular, the technique of fitting sparse coders to these internal layer activations has allowed for obtaining explainable neurons, which typically do not exist in transformers thanks to the phenomenon of superposition.</p>
<p>In working on classifiers based on natural language, I’ve become interested in how neural classifiers can be compared with each other. Since my work has primarily been focused on clinical text (e.g.&nbsp;discharge summaries) and clinical outcomes (e.g.&nbsp;30-day mortality), I’ll narrate my motivation through the lens of the healthcare domain, which I consider to be a high-stakes setting for interpretability performance. If I have two models for patient mortality prediction, one developed from fine-tuning transformer A on Dataset 1, and another developed from fine-tuning transformer B on Dataset 2, how can I compare these two models, ultimately to figure out which one has more signal-based computational processes, performs more robustly, and understands more reasons for death?</p>
<p>If I had used two statistical models instead of two transformer models, this comparison would be easy. I could first compare the feature sets used by my models (e.g.&nbsp;patient age, number of diagnoses, length of stay) and then, for overlapping features, compare the coefficients being used. There are methods available for defining and sometimes visualizing the regions of disagreement between these two models.</p>
<p>Thanks to sparse coding algorithms for transformer interpretability, we now have a toolkit for comparing two models based on the features that they use in prediction. Off the bat, this is still a difficult problem because obtaining the features that are intrinsic to a trained architecture requires 1) obtaining a large, comprehensive set of example texts with activations, 2) choosing a layer or layers within the model to focus on, and 3) training a sparse crosscoder using that dataset and those layers. Assuming this has already been done for both models (separately), there’s no obvious way to compare the feature sets of the two models.</p>
<p>In this technical blog post, I outline my thought process for this problem, starting with the much more basic question of finding a set of features that are shared between two residual stream layers of the same transformer model. There are some interesting results from just looking at gpt-2 small, for which residual stream SAE’s have already been trained and made available through the SAELens library.</p>
<hr>
<section id="a-first-pass-approach-correlated-features" class="level2">
<h2 class="anchored" data-anchor-id="a-first-pass-approach-correlated-features">A first-pass approach: correlated features</h2>
<p>First, let’s formalizing a recurring feature as a pair of latents that exist in separate models or separate layers of the same model but represent the same concept. Already this definition is hairy, because SAE latents are generally non-atomic and have somewhat subjective interpretations, which are typically written by an LLM. However, a baseline strategy for identifying whether two latents represent the same concept is to correlate their activations, using a large corpus of documents. It’s immediately clear that searching for recurring features will be very time and memory intensive, particularly for SAE’s with large expansion factors and low activation densities. The main computational trick I use here is calculating correlation online (so that batches can be deleted after activations are computed) and storing the <span class="math inline">\(D_1\)</span> x <span class="math inline">\(D_2\)</span> correlation matrix in sparse matrix form, so as to not eat too much memory. Together they seem to be enough for obtaining all the features with <span class="math inline">\(&gt;0.9\)</span> correlation between two layers of gpt-2 small on a single NVIDIA A6000 GPU.</p>
<section id="examples-of-correlated-features" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-correlated-features">Examples of correlated features</h3>
<p>In my preliminary work, I mainly focus on layers 2 and 11 of gpt2-small, using the SAE trained on these layers by Joseph Bloom (https://www.neuronpedia.org/gpt2-small/res-jb). I extracted 50,000 texts from the wiki-text dataset, obtained latent activations for both layers on the full dataset, and extracted all feature pairs with greater than 0.9 correlation. Here are some of the features that I found most interesting. Generally, the notation ‘X/Y’ refers to the latent with index Y within layer X of the model.</p>
<p>Latent 2/13673 has the Neuronpedia auto-interpretation <code>mentions of the Android operating system.</code> 11/11487 is <code>mentions of the term Android, likely in the context of technology.</code> Their correlation is 0.994 on the dataset I’ve created, but producing examples in which the later feature activates but the earlier one doesn’t is simple. The prompt <code>Google's operating system for phones</code> causes no activations in the earlier feature, but activates on all but one token in the second feature.</p>
<div style="display: flex; justify-content: center; gap: 20px;">
<p><iframe src="https://neuronpedia.org/gpt2-small/2-res-jb/13673?embed=true&amp;embedexplanation=true&amp;embedplots=true&amp;embedtest=true" width="48%" height="300" frameborder="0" allowfullscreen=""> </iframe></p>
<p><iframe src="https://neuronpedia.org/gpt2-small/11-res-jb/11487?embed=true&amp;embedexplanation=true&amp;embedplots=true&amp;embedtest=true" width="48%" height="300" frameborder="0" allowfullscreen=""> </iframe></p>
</div>
<p>Another example pair is 2/10606 (words related to software and technology) and 11/21276 (specific references to software programs). Entering <code>the code was written</code> does not activate the first feature, but it activates the second feature on all tokens.</p>
<p>So within two layers of the same model, one recurring archetype of <code>recurring feature</code> is a feature becoming more semantic and less keyword-based in the later layer than the earlier layer; we could even think of this as the same feature gaining higher recall, but I think that would be inaccurate. Overall, this pattern of feature development makes sense, especially given that we’re starting with a very early layer – the layers in-between allow for processing context and making a more intelligent representation, which is why the later layer is less literal. Anecdotally, I was slightly surprised by how many of the earlier features only activate for a few keywords, e.g.&nbsp;only activating on the word ‘Android.’ It would be interesting to visualize the number of unique activating keywords for each of these latents, and how that distribution differs between layers, and whether this distribution tends to converge to a degenerate one after (e.g.) 6 or 7 MHA layers.</p>
</section>
</section>
<section id="similarity-in-decoder-weights" class="level2">
<h2 class="anchored" data-anchor-id="similarity-in-decoder-weights">Similarity in decoder weights</h2>
<p>Each attention block and MLP block transforms its input, but because of the residual connection between these blocks, it’s intuitively possible that if one feature persists between two layers, it will manifest in the same subspace between them. I don’t have a formal reasoning for why this necessarily will be the case, but I believe that it can be the case. Consider the (unrealistic) example in which one feature emerges in layer L of the model but does not superimpose with any other feature. Imagine that we discover it through a decoder vector of <span class="math inline">\(a \vec e_i + b \vec e_j + c \vec e_k\)</span>, where <span class="math inline">\(a,b,c\)</span> are arbitrary reals and <span class="math inline">\(i,j,k\)</span> are arbitrary integers. If the attention block and MLP block of layer L+1 both somehow do not write to indices <span class="math inline">\(i,j,k\)</span> of the residual stream, then we can expect this feature to recur in layer <span class="math inline">\(L+1\)</span>, and with the same decoder vector.</p>
<p>Again, this reasoning is meant to motivate a hypothesis rather than prove a method. An empirical validation comes from checking the dot products of correlated features, and comparing this distribution to that of a simulated set of randomly-chosen feature pairs. There’s a clear separation between the two distributions, suggesting that we can hopefully identify recurring features without even needing to obtain activations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/null_alt_dot.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Decoder weight dot products</figcaption>
</figure>
</div>
<p>I hypothesized that two instances of a recurrent feature would have a high dot product when projected into the unembedding space (e.g.&nbsp;logit vectors), and for that reason I also obtained the distribution of bilinear products <span class="math inline">\(v_1 W_U W_U^T v_2\)</span>, which again seem to be separated, but not as much as the dot product distributions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/null_alt_bilinear.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Decoder weight bilinear products</figcaption>
</figure>
</div>
<p>The natural application is to replace my 1st approach for discovering recurring features (high correlation on activation sets) with a decoder weights similarity search. The advantage of not using activations is that we don’t have to worry about bias (e.g.&nbsp;using a dataset that only represents a certain set of concepts, e.g.&nbsp;the IMDb dataset) or the computational bottleneck of performing inference over a large dataset and two relatively-large models.</p>
<p>If our two decoder matrices are <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> (where each row is a decoder vector), we’re effectively searching for the largest elements in <span class="math inline">\(D_1 D_2^T\)</span>. Computing <span class="math inline">\(D_1 D_2^T\)</span> is not necessary here, as computational shortcuts exist for finding vector pairs of high similarity. I opted to use FAISS [https://github.com/facebookresearch/faiss] from Facebook AI Research, which reduces the number of operations needed for this task by first grouping all the vectors in one dataset (matrix) into clusters, then only computing similarity scores between a vector and the ones in the nearest cluster.</p>
<p>Results are a mixed-bag. The largest similarity score was between 2/9515 and 11/17596. The autointerps are respectively <code>words related to organization and realization</code> and <code>phrases related to utilizing tools or resources.</code> Upon inspection of the frequently activating keywords, these autointerps are incorrect. Both features are moderately-strong detectors of British English being used.</p>
<ul>
<li>2/9515 activating keywords
<ul>
<li>organisation</li>
<li>realise</li>
<li>recognised</li>
<li>emphasising</li>
</ul></li>
<li>11/17596
<ul>
<li>manoeuvure</li>
<li>utilising</li>
<li>focussed</li>
<li>armour</li>
<li>999 (this is the emergency phone number in the UK, as well as many other countries in the old British commonwealth)</li>
<li>sceptical</li>
</ul></li>
</ul>
<p>Interestingly, this feature pair did not have a correlation greater than 0.9, even though it is (in my opinion) a pretty good example of a recurring feature.</p>
<p>The second most similar feature pair was 2/21514 and 11/1652; these features respectively have activation densities of 5.6% and 4.0%, and don’t have any legitimate interpretations. They do share a most-activating token, which is a string of special characters <code>âĢİ.</code> Spooky. This feature pair also was not discovered by activation correlation.</p>
<p>The third most similar feature pair was 2/5186 and 11/5362, which are <code>the word Wilson with varying levels of specificity</code> and <code>the word Wilson in various contexts.</code> This one makes a lot more sense, and also was discovered by activation correlation (0.954). Again, the later feature is smarter, as evidenced by only it activating on <code>Seahawks Quarterback Russell.</code></p>
<p>It’s useful to see just how close two features can be. Between layers 2 and 11, the largest similarity was 0.8362. If we investigate (e.g.) layers 7 and 8, can we find the same feature in both layers?</p>
<p>The answer turns out to be: yes, there are several features that seem to be exactly preserved between layers, and it’s quite a good sign that separately trained SAE’s find the same subspace here. Here I display a scatterplot of the decoder weights for 7/8598 and 8/6955, which have a similarity metric of 1.0000. These features are both activated on &lt;|endoftext|&gt;.</p>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/decoder_weight_7_8_scatter.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Full decoder weight scatterplot</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/decoder_weight_7_8_scatter_zoom.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>0.01-0.99 quantile decoder weight scatterplot</figcaption>
</figure>
</div>
</div>
</div>
<p>Interestingly, there seem to be multiple features in layers 7 and 8 that activate on &lt;|endoftext|&gt;, and all of them seem to have roughly the same decoder weights (similarity score of 1.0).</p>
</section>
<section id="visualizing-representation-comparison" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-representation-comparison">Visualizing Representation Comparison</h2>
<p>NB: As I drift from comparing different layers within the same model to comparing different layers of different models, I’m going to generalize to the idea of comparing neural representations.</p>
<p>We saw a lot of features copied from layers 7 to 8, and not a lot between layers 2 and 11. This makes sense, as layers 2 and 11 should be much different as representations than layers 7 and 8. It would be useful to have a similarity metric between two representations that expresses how different their featurizations are. This ties back to the original ideal of this project, which is to have a method that quantifies the similarity between two transformer computational graphs, based on the features they both identify (making the rather strong assumption, at least within our scope, that both of these representations have sparse replacement models available).</p>
<p>A first-pass at visualizing the similarity between two layers is to obtain, for each latent in one layer, the largest similarity score with any latent in the other layer. Mathematically, if <span class="math inline">\(W_{d,L_1}\)</span> and <span class="math inline">\(W_{d,L_2}\)</span> are the decoder matrices (in $^{F x D_H}, where each row is a feature, define</p>
<p><span class="math display">\[
\rho_{L_1, L_2} (i) = max_{j \in |F|} W_{d,1}[i,:]^T W_{d,2}[j,:]
\]</span></p>
<p>If <span class="math inline">\(\rho_{L_1, L_2} (i)\)</span> is high for all <span class="math inline">\(i \in |F|\)</span>, we can comfortably say that <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> share similar featurizations. We can display the vector <span class="math inline">\(\vec \rho(L_1, L_2) = (\rho_{L_1, L_2} (1)... \rho_{L_1, L_2} (F))\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/latent_max_sims.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Histograms of latent max similarities</figcaption>
</figure>
</div>
<p>Here, I plot the <span class="math inline">\(\rho\)</span> vectors between layers 1, 5, 8, and 11, blacking out the diagonal because it is obviously just 1.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/grid_layer_vectors.png" class="img-fluid figure-img"></p>
<figcaption>Histogram grid of latent max similarities: 1, 5, 8, 11</figcaption>
</figure>
</div>
</section>
<section id="model-diffing-with-decoder-weights" class="level2">
<h2 class="anchored" data-anchor-id="model-diffing-with-decoder-weights">Model diffing with decoder weights</h2>
<p>Overall, I like the decoder-weights approach a lot more for finding persistent features within a model, but there’s no obvious way to translate it for different residual streams. One application is to focus on two different models that originate from the same base model, in the hopes that the residual streams are similar enough that the same feature in both models would occupy similar subspaces. This is a core component of <a href="https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html">model diffing</a>, which Anthropic has already briefly used in order to show differences between base and fine-tuned models. I think their work on this is quite exciting but somewhat limited to feature exploration, whereas I’m more interested in directly comparing models by understanding the features that they use.</p>
<p>At this point, it’s important to recall that autointerp/activation tracking has been central to verifying that two features really are the same, and for that I credit Neuronpedia and all the SAE trainers who have populated it. For the next step in this blog, I’ll need to diff two models that originate from the same base model, and they’ll both need to already have SAE’s. The pretrained SAE’s library in <a href="https://github.com/jbloomAus/SAELens">sae lens</a> is quite extensive, although I believe that the only pair of models here originating from the same base are Base Llama-3-8b and Deepseek-R1-distilled Llama-3-8b. Respectively, residual stream layers 18 and 25 are available, so I visualize the feature relationships here.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/distill_llama.png" class="img-fluid figure-img"></p>
<figcaption><span class="math inline">\(\rho\)</span> between Distill-Llama and Base-Llama</figcaption>
</figure>
</div>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>I meant for this to be a longer blog post that culminated in a proposed methodology for model/representation comparison with sparse-coded feature weights. I thought it would make sense to put these explorations and thought experiments into an earlier (first) technical blog post, then later update it with new results once I’ve trained a pair of good SAE’s based on the language models I’m using.</p>
<p>I’m primarily interested in an approach that is empirically verifiable, which (to me) means that I can set up toy experiments with small transformers where the feature similarity is either analytically tractable, or a good hypothesis can be made as to what they’ll be. An even earlier pass would be to build off the work of <a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a>, since the transformer architecture itself can be ignored to start.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Finding features that persist across layers and models"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Shariar Vaez-Ghaemi"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-06-25"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>In statistical prediction settings, it's known that the best models are the ones with the most important features, rather than the most complicated architectures. Choosing good features for your prediction model can be easy, thanks to regularization techniques and the expressiveness of deep architectures. </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>But while feature engineering has become an abandoned art, feature awareness is still crucial. In the race to understand the computational paths taken by a transformer in computing a next-token prediction, mechanistic interpretability researchers have put effort into extracting concept subspaces within the intermediate representations of these transformer architectures. In particular, the technique of fitting sparse coders to these internal layer activations has allowed for obtaining explainable neurons, which typically do not exist in transformers thanks to the phenomenon of superposition.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>In working on classifiers based on natural language, I've become interested in how neural classifiers can be compared with each other. Since my work has primarily been focused on clinical text (e.g. discharge summaries) and clinical outcomes (e.g. 30-day mortality), I'll narrate my motivation through the lens of the healthcare domain, which I consider to be a high-stakes setting for interpretability performance. If I have two models for patient mortality prediction, one developed from fine-tuning transformer A on Dataset 1, and another developed from fine-tuning transformer B on Dataset 2, how can I compare these two models, ultimately to figure out which one has more signal-based computational processes, performs more robustly, and understands more reasons for death?</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>If I had used two statistical models instead of two transformer models, this comparison would be easy. I could first compare the feature sets used by my models (e.g. patient age, number of diagnoses, length of stay) and then, for overlapping features, compare the coefficients being used. There are methods available for defining and sometimes visualizing the regions of disagreement between these two models. </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>Thanks to sparse coding algorithms for transformer interpretability, we now have a toolkit for comparing two models based on the features that they use in prediction. Off the bat, this is still a difficult problem because obtaining the features that are intrinsic to a trained architecture requires 1) obtaining a large, comprehensive set of example texts with activations, 2) choosing a layer or layers within the model to focus on, and 3) training a sparse crosscoder using that dataset and those layers. Assuming this has already been done for both models (separately), there's no obvious way to compare the feature sets of the two models. </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>In this technical blog post, I outline my thought process for this problem, starting with the much more basic question of finding a set of features that are shared between two residual stream layers of the same transformer model. There are some interesting results from just looking at gpt-2 small, for which residual stream SAE's have already been trained and made available through the SAELens library. </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## A first-pass approach: correlated features</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>First, let's formalizing a recurring feature as a pair of latents that exist in separate models or separate layers of the same model but represent the same concept. Already this definition is hairy, because SAE latents are generally non-atomic and have somewhat subjective interpretations, which are typically written by an LLM. However, a baseline strategy for identifying whether two latents represent the same concept is to correlate their activations, using a large corpus of documents. It's immediately clear that searching for recurring features will be very time and memory intensive, particularly for SAE's with large expansion factors and low activation densities. The main computational trick I use here is calculating correlation online (so that batches can be deleted after activations are computed) and storing the $D_1$ x $D_2$ correlation matrix in sparse matrix form, so as to not eat too much memory. Together they seem to be enough for obtaining all the features with $&gt;0.9$ correlation between two layers of gpt-2 small on a single NVIDIA A6000 GPU. </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="fu">### Examples of correlated features</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>In my preliminary work, I mainly focus on layers 2 and 11 of gpt2-small, using the SAE trained on these layers by Joseph Bloom (https://www.neuronpedia.org/gpt2-small/res-jb). I extracted 50,000 texts from the wiki-text dataset, obtained latent activations for both layers on the full dataset, and extracted all feature pairs with greater than 0.9 correlation. Here are some of the features that I found most interesting. Generally, the notation 'X/Y' refers to the latent with index Y within layer X of the model.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>Latent 2/13673 has the Neuronpedia auto-interpretation <span class="in">`mentions of the Android operating system.`</span> 11/11487 is <span class="in">` mentions of the term Android, likely in the context of technology.`</span> Their correlation is 0.994 on the dataset I've created, but producing examples in which the later feature activates but the earlier one doesn't is simple. The prompt <span class="in">`Google's operating system for phones`</span> causes no activations in the earlier feature, but activates on all but one token in the second feature.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> style</span><span class="op">=</span><span class="st">"display: flex; justify-content: center; gap: 20px;"</span><span class="dt">&gt;</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">iframe</span><span class="ot"> </span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ot">    src</span><span class="op">=</span><span class="st">"https://neuronpedia.org/gpt2-small/2-res-jb/13673?embed=true</span><span class="er">&amp;</span><span class="st">embedexplanation=true</span><span class="er">&amp;</span><span class="st">embedplots=true</span><span class="er">&amp;</span><span class="st">embedtest=true"</span><span class="ot"> </span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ot">    width</span><span class="op">=</span><span class="st">"48%"</span><span class="ot"> </span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ot">    height</span><span class="op">=</span><span class="st">"300"</span><span class="ot"> </span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ot">    frameborder</span><span class="op">=</span><span class="st">"0"</span><span class="ot"> </span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ot">    allowfullscreen</span><span class="dt">&gt;</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">iframe</span><span class="dt">&gt;</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">iframe</span><span class="ot"> </span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="ot">    src</span><span class="op">=</span><span class="st">"https://neuronpedia.org/gpt2-small/11-res-jb/11487?embed=true</span><span class="er">&amp;</span><span class="st">embedexplanation=true</span><span class="er">&amp;</span><span class="st">embedplots=true</span><span class="er">&amp;</span><span class="st">embedtest=true"</span><span class="ot"> </span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="ot">    width</span><span class="op">=</span><span class="st">"48%"</span><span class="ot"> </span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="ot">    height</span><span class="op">=</span><span class="st">"300"</span><span class="ot"> </span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ot">    frameborder</span><span class="op">=</span><span class="st">"0"</span><span class="ot"> </span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ot">    allowfullscreen</span><span class="dt">&gt;</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">iframe</span><span class="dt">&gt;</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>Another example pair is 2/10606 (words related to software and technology) and 11/21276 (specific references to software programs). Entering <span class="in">`the code was written`</span> does not activate the first feature, but it activates the second feature on all tokens. </span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>So within two layers of the same model, one recurring archetype of <span class="in">`recurring feature`</span> is a feature becoming more semantic and less keyword-based in the later layer than the earlier layer; we could even think of this as the same feature gaining higher recall, but I think that would be inaccurate. Overall, this pattern of feature development makes sense, especially given that we're starting with a very early layer -- the layers in-between allow for processing context and making a more intelligent representation, which is why the later layer is less literal. Anecdotally, I was slightly surprised by how many of the earlier features only activate for a few keywords, e.g. only activating on the word 'Android.' It would be interesting to visualize the number of unique activating keywords for each of these latents, and how that distribution differs between layers, and whether this distribution tends to converge to a degenerate one after (e.g.) 6 or 7 MHA layers.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="fu">## Similarity in decoder weights</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>Each attention block and MLP block transforms its input, but because of the residual connection between these blocks, it's intuitively possible that if one feature persists between two layers, it will manifest in the same subspace between them. I don't have a formal reasoning for why this necessarily will be the case, but I believe that it can be the case. Consider the (unrealistic) example in which one feature emerges in layer L of the model but does not superimpose with any other feature. Imagine that we discover it through a decoder vector of $a \vec e_i + b \vec e_j + c \vec e_k$, where $a,b,c$ are arbitrary reals and $i,j,k$ are arbitrary integers. If the attention block and MLP block of layer L+1 both somehow do not write to indices $i,j,k$ of the residual stream, then we can expect this feature to recur in layer $L+1$, and with the same decoder vector.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>Again, this reasoning is meant to motivate a hypothesis rather than prove a method. An empirical validation comes from checking the dot products of correlated features, and comparing this distribution to that of a simulated set of randomly-chosen feature pairs. There's a clear separation between the two distributions, suggesting that we can hopefully identify recurring features without even needing to obtain activations. </span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="al">![Decoder weight dot products](plots/null_alt_dot.png)</span>{width=50%}</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>I hypothesized that two instances of a recurrent feature would have a high dot product when projected into the unembedding space (e.g. logit vectors), and for that reason I also obtained the distribution of bilinear products $v_1 W_U W_U^T v_2$, which again seem to be separated, but not as much as the dot product distributions.</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="al">![Decoder weight bilinear products](plots/null_alt_bilinear.png)</span>{width=50%}</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>The natural application is to replace my 1st approach for discovering recurring features (high correlation on activation sets) with a decoder weights similarity search. The advantage of not using activations is that we don't have to worry about bias (e.g. using a dataset that only represents a certain set of concepts, e.g. the IMDb dataset) or the computational bottleneck of performing inference over a large dataset and two relatively-large models.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>If our two decoder matrices are $D_1$ and $D_2$ (where each row is a decoder vector), we're effectively searching for the largest elements in $D_1 D_2^T$. Computing $D_1 D_2^T$ is not necessary here, as computational shortcuts exist for finding vector pairs of high similarity. I opted to use FAISS <span class="co">[</span><span class="ot">https://github.com/facebookresearch/faiss</span><span class="co">]</span> from Facebook AI Research, which reduces the number of operations needed for this task by first grouping all the vectors in one dataset (matrix) into clusters, then only computing similarity scores between a vector and the ones in the nearest cluster. </span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>Results are a mixed-bag. The largest similarity score was between 2/9515 and 11/17596. The autointerps are respectively <span class="in">`words related to organization and realization`</span> and <span class="in">`phrases related to utilizing tools or resources.`</span> Upon inspection of the frequently activating keywords, these autointerps are incorrect. Both features are moderately-strong detectors of British English being used. </span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>2/9515 activating keywords</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>organisation</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>realise</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>recognised</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>emphasising</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>11/17596</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>manoeuvure</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>utilising</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>focussed</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>armour</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>999 (this is the emergency phone number in the UK, as well as many other countries in the old British commonwealth)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>sceptical</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>Interestingly, this feature pair did not have a correlation greater than 0.9, even though it is (in my opinion) a pretty good example of a recurring feature.</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>The second most similar feature pair was 2/21514 and 11/1652; these features respectively have activation densities of 5.6% and 4.0%, and don't have any legitimate interpretations. They do share a most-activating token, which is a string of special characters <span class="in">`âĢİ.`</span> Spooky. This feature pair also was not discovered by activation correlation.</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>The third most similar feature pair was 2/5186 and 11/5362, which are <span class="in">`the word Wilson with varying levels of specificity`</span> and <span class="in">`the word Wilson in various contexts.`</span> This one makes a lot more sense, and also was discovered by activation correlation (0.954). Again, the later feature is smarter, as evidenced by only it activating on <span class="in">`Seahawks Quarterback Russell.`</span> </span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>It's useful to see just how close two features can be. Between layers 2 and 11, the largest similarity was 0.8362. If we investigate (e.g.) layers 7 and 8, can we find the same feature in both layers?</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>The answer turns out to be: yes, there are several features that seem to be exactly preserved between layers, and it's quite a good sign that separately trained SAE's find the same subspace here. Here I display a scatterplot of the decoder weights for 7/8598 and 8/6955, which have a similarity metric of 1.0000. These features are both activated on &lt;|endoftext|&gt;. </span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>:::{.columns}</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="al">![Full decoder weight scatterplot](plots/decoder_weight_7_8_scatter.png)</span>{width=100%}</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="al">![0.01-0.99 quantile decoder weight scatterplot](plots/decoder_weight_7_8_scatter_zoom.png)</span>{width=100%}</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>Interestingly, there seem to be multiple features in layers 7 and 8 that activate on &lt;|endoftext|&gt;, and all of them seem to have roughly the same decoder weights (similarity score of 1.0).  </span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualizing Representation Comparison</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>NB: As I drift from comparing different layers within the same model to comparing different layers of different models, I'm going to generalize to the idea of comparing neural representations. </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>We saw a lot of features copied from layers 7 to 8, and not a lot between layers 2 and 11. This makes sense, as layers 2 and 11 should be much different as representations than layers 7 and 8. It would be useful to have a similarity metric between two representations that expresses how different their featurizations are. This ties back to the original ideal of this project, which is to have a method that quantifies the similarity between two transformer computational graphs, based on the features they both identify (making the rather strong assumption, at least within our scope, that both of these representations have sparse replacement models available).</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>A first-pass at visualizing the similarity between two layers is to obtain, for each latent in one layer, the largest similarity score with any latent in the other layer. Mathematically, if $W_{d,L_1}$ and $W_{d,L_2}$ are the decoder matrices (in $\mathcal{R}^{F x D_H}, where each row is a feature, define</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>\rho_{L_1, L_2} (i) = max_{j \in |F|} W_{d,1}<span class="co">[</span><span class="ot">i,:</span><span class="co">]</span>^T W_{d,2}<span class="co">[</span><span class="ot">j,:</span><span class="co">]</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>If $\rho_{L_1, L_2} (i)$ is high for all $i \in |F|$, we can comfortably say that $L_1$ and $L_2$ share similar featurizations. We can display the vector $\vec \rho(L_1, L_2) = (\rho_{L_1, L_2} (1)... \rho_{L_1, L_2} (F))$</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="al">![Histograms of latent max similarities](plots/latent_max_sims.png)</span>{width=50%}</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>Here, I plot the $\rho$ vectors between layers 1, 5, 8, and 11, blacking out the diagonal because it is obviously just 1.</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a><span class="al">![Histogram grid of latent max similarities: 1, 5, 8, 11](plots/grid_layer_vectors.png)</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model diffing with decoder weights</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>Overall, I like the decoder-weights approach a lot more for finding persistent features within a model, but there's no obvious way to translate it for different residual streams. One application is to focus on two different models that originate from the same base model, in the hopes that the residual streams are similar enough that the same feature in both models would occupy similar subspaces. This is a core component of <span class="co">[</span><span class="ot">model diffing</span><span class="co">](https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html)</span>, which Anthropic has already briefly used in order to show differences between base and fine-tuned models. I think their work on this is quite exciting but somewhat limited to feature exploration, whereas I'm more interested in directly comparing models by understanding the features that they use. </span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>At this point, it's important to recall that autointerp/activation tracking has been central to verifying that two features really are the same, and for that I credit Neuronpedia and all the SAE trainers who have populated it. For the next step in this blog, I'll need to diff two models that originate from the same base model, and they'll both need to already have SAE's. The pretrained SAE's library in <span class="co">[</span><span class="ot">sae lens</span><span class="co">](https://github.com/jbloomAus/SAELens)</span> is quite extensive, although I believe that the only pair of models here originating from the same base are Base Llama-3-8b and Deepseek-R1-distilled Llama-3-8b. Respectively, residual stream layers 18 and 25 are available, so I visualize the feature relationships here.</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="al">![$\rho$ between Distill-Llama and Base-Llama](plots/distill_llama.png)</span></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next steps</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>I meant for this to be a longer blog post that culminated in a proposed methodology for model/representation comparison with sparse-coded feature weights. I thought it would make sense to put these explorations and thought experiments into an earlier (first) technical blog post, then later update it with new results once I've trained a pair of good SAE's based on the language models I'm using.</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>I'm primarily interested in an approach that is empirically verifiable, which (to me) means that I can set up toy experiments with small transformers where the feature similarity is either analytically tractable, or a good hypothesis can be made as to what they'll be. An even earlier pass would be to build off the work of <span class="co">[</span><span class="ot">Toy Models of Superposition</span><span class="co">](https://transformer-circuits.pub/2022/toy_model/index.html)</span>, since the transformer architecture itself can be ignored to start. </span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>